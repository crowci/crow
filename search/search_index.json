{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Crow is a Continuous Integration &amp; Continuous Delivery (CI/CD)<sup>1</sup> application. It is designed to be lightweight, simple to use and fast. </p> <p>A typical CI/CD pipeline usually includes the following steps:</p> <ol> <li>Running tests and linters</li> <li>Building an application</li> <li>Deploying an application</li> </ol> <p>However, these days CI/CD apps are also often used as flexible task schedulers with central logging and can be tailored to pretty much any use case.</p>"},{"location":"#containers-at-the-core","title":"Containers at the core","text":"<p>Crow is only using containers to run pipelines. This stands in contrast to other common CI/CD apps like GitHub Actions, GitLab Runner or Jenkins. While these can also execute pipelines in containers if desired, they are \"Host-first\", i.e. by default they run pipelines directly on a VM and are restricted to the OS on that VM.</p> <p>Crow in contrast only uses containers and is hence operating-system agnostic, i.e. it does not matter which OS is running on the host which is executing the pipelines.</p> <p>With respect to well-known CI/CD providers, Crow is mostly comparable to Circle CI, which is also using a container-only approach.</p>"},{"location":"#target-audience","title":"Target audience","text":"<p>Crow is FOSS, lightweight and shines when being self-hosted in private environments<sup>3</sup>. It has a very small footprint (&lt; 200 MB memory consumption) in idle and can even be installed and practically used on a Raspberry Pi with 4GB (or even less, depending on what the builds will do).</p> <p>Crow uses a SQLite by default and can do a lot with in during runtime. If you are planning a bigger instance with multiple (active) users and &gt; 20 active repos, it is recommended to configure it with a Postgres or MariaDB database for performance reasons.<sup>2</sup></p>"},{"location":"#history","title":"History","text":"<p>Crow has been forked from Woodpecker CI (v3.0.0) in January 2025 from the previous Woodpecker maintainer @pat-s. Motivation was built around improving infrastructure-related processes of the project (releases, docs, governance) which mainly found no agreement in previous discussions in Woodpecker.</p> <p>Woodpecker itself is a fork of Drone CI (v0.8.91) in April 2019 with the first standalone version being release on September 9th 2019.</p>"},{"location":"#license","title":"License","text":"<p>Due to being a fork of Woodpecker and Drone, Crow is licensed under the Apache 2.0 license.</p> <ol> <li> <p>This RedHat blog post explains the concept of CI/CD in more detail. \u21a9</p> </li> <li> <p>This is primarily because Crow (still) stores pipeline logs in the DB. A refactoring to storing these outside the DB by default is planned but now yet implemented.\u00a0\u21a9</p> </li> <li> <p>Crow can also be run in public which is successfully shown on Codeberg which uses Woodpecker (which Crow was forked from) as the main CI/CD engine on its platform. Doing so however requires more administration RBAC-work on the admin side (giving permissions, enforcing trusted plugins, caring about secret exposure) than running Crow within a closed private environment.\u00a0\u21a9</p> </li> </ol>"},{"location":"02-configuration/","title":"Configuration","text":""},{"location":"02-configuration/#server","title":"Server","text":""},{"location":"02-configuration/#agent","title":"Agent","text":""},{"location":"03-plugins/","title":"Plugins","text":""},{"location":"031-usage/","title":"Usage","text":""},{"location":"031-usage/#pipelines","title":"Pipelines","text":""},{"location":"031-usage/#essential-elements","title":"Essential elements","text":""},{"location":"031-usage/#secrets","title":"Secrets","text":""},{"location":"031-usage/#environment-variables","title":"Environment Variables","text":""},{"location":"031-usage/#optional-elements","title":"Optional elements","text":""},{"location":"031-usage/#registries","title":"Registries","text":""},{"location":"031-usage/#services","title":"Services","text":"<p>Woodpecker provides a services section in the YAML file used for defining service containers. The below configuration composes database and cache containers.</p> <p>Services are accessed using custom hostnames. In the example below, the MySQL service is assigned the hostname <code>database</code> and is available at <code>database:3306</code>.</p> <pre><code>steps:\n  - name: build\n    image: golang\n    commands:\n      - go build\n      - go test\n\nservices:\n  - name: database\n    image: mysql\n\n  - name: cache\n    image: redis\n</code></pre> <p>You can define a port and a protocol explicitly:</p> <pre><code>services:\n  - name: database\n    image: mysql\n    ports:\n      - 3306\n\n  - name: wireguard\n    image: wg\n    ports:\n      - 51820/udp\n</code></pre>"},{"location":"031-usage/#configuration","title":"Configuration","text":"<p>Service containers generally expose environment variables to customize service startup such as default usernames, passwords and ports. Please see the official image documentation to learn more.</p> <pre><code> services:\n   - name: database\n     image: mysql\n+    environment:\n+      - MYSQL_DATABASE=test\n+      - MYSQL_ALLOW_EMPTY_PASSWORD=yes\n\n   - name: cache\n     image: redis\n</code></pre>"},{"location":"031-usage/#detachment","title":"Detachment","text":"<p>Service and long running containers can also be included in the pipeline section of the configuration using the detach parameter without blocking other steps. This should be used when explicit control over startup order is required.</p> <pre><code> steps:\n   - name: build\n     image: golang\n     commands:\n       - go build\n       - go test\n\n   - name: database\n     image: redis\n+    detach: true\n\n   - name: test\n     image: golang\n     commands:\n       - go test\n</code></pre> <p>Containers from detached steps will terminate when the pipeline ends.</p>"},{"location":"031-usage/#initialization","title":"Initialization","text":"<p>Service containers require time to initialize and begin to accept connections. If you are unable to connect to a service you may need to wait a few seconds or implement a backoff.</p> <pre><code> steps:\n   - name: test\n     image: golang\n     commands:\n+      - sleep 15\n       - go get\n       - go test\n\n services:\n   - name: database\n     image: mysql\n</code></pre>"},{"location":"031-usage/#full-services-example","title":"Full \"Services\" example","text":"<pre><code>services:\n  - name: database\n    image: mysql\n    environment:\n      - MYSQL_DATABASE=test\n      - MYSQL_ROOT_PASSWORD=example\nsteps:\n  - name: get-version\n    image: ubuntu\n    commands:\n      - ( apt update &amp;&amp; apt dist-upgrade -y &amp;&amp; apt install -y mysql-client 2&gt;&amp;1 )&gt; /dev/null\n      - sleep 30s # need to wait for mysql-server init\n      - echo 'SHOW VARIABLES LIKE \"version\"' | mysql -u root -h database test -p example\n</code></pre>"},{"location":"031-usage/#volumes","title":"Volumes","text":""},{"location":"031-usage/#cron-jobs","title":"CRON jobs","text":""},{"location":"031-usage/#project-settings","title":"Project settings","text":""},{"location":"031-usage/#miscellaneous","title":"Miscellaneous","text":""},{"location":"031-usage/#warnings-error-linter","title":"Warnings &amp; Error Linter","text":""},{"location":"031-usage/#advanced","title":"Advanced","text":""},{"location":"04-development/","title":"Development","text":""},{"location":"04-development/#images","title":"Images","text":""},{"location":"04-development/#documentation","title":"Documentation","text":""},{"location":"06-community/","title":"Community","text":""},{"location":"configuration/","title":"Configuration","text":"<p>All configuration is done through via environment variables. The pages here describe the configuration of Server, Agent and the optional Autoscaler in greater detail.</p> <p>A complete list of all existing environment variables is available at All environment variables.</p>"},{"location":"configuration/env-vars/","title":"Server","text":"Name Description Value <code>WOODPECKER_BACKEND_K8S_NAMESPACE</code> The k8s namespace to execute pipelines in <code>crow</code> <code>WOODPECKER_BACKEND_K8S_POD_ANNOTATIONS</code> Additional annotations to apply to worker Pods. Must be a YAML object, e.g. <code>{\"example.com/test-annotation\":\"test-value\"}</code>. <code>WOODPECKER_BACKEND_K8S_POD_LABELS_ALLOW_FROM_STEP</code> Determines if Pod annotations can be defined from a step's backend options. <code>false</code> <code>WOODPECKER_BACKEND_K8S_POD_LABELS</code> Additional labels to apply to worker Pods. Must be a YAML object, e.g. <code>{\"example.com/test-label\":\"test-value\"}</code> <code>WOODPECKER_BACKEND_K8S_POD_NODE_SELECTOR</code> Additional node selector to apply to worker pods. Must be a YAML object, e.g. <code>{\"topology.kubernetes.io/region\":\"eu-central-1\"}</code> <code>WOODPECKER_BACKEND_K8S_PULL_SECRET_NAMES</code> Secret names to pull images from private repositories. <code>WOODPECKER_BACKEND_K8S_SECCTX_NONROOT</code> Whether containers must be run as a non-root user. <code>false</code> <code>WOODPECKER_BACKEND_K8S_STORAGE_CLASS</code> The storage class to use for the temporary pipeline volume. <code>WOODPECKER_BACKEND_K8S_STORAGE_RWX</code> Whether a RWX should be used for the temporary pipeline volume. If false, RWO is used instead. <code>true</code> <code>WOODPECKER_BACKEND_K8S_STORAGE_CLASS</code> The storage class to use for the pipeline volume. <code>WOODPECKER_BACKEND_K8S_VOLUME_SIZE</code> The volume size of the temporary pipeline volume. <code>10G</code> <code>WOODPECKER_BACKEND_LOCAL_TEMP_DIR</code> Directory in which pipelines are executed <code>$TMPDIR</code>"},{"location":"configuration/env-vars/#agent","title":"Agent","text":""},{"location":"configuration/server/","title":"Server","text":""},{"location":"configuration/server/#forge-user-configuration","title":"Forge &amp; User configuration","text":"<p>Crow can only be used in combination with a \"Forge\" (i.e. Git Provider). The following ones are currently supported:</p> <ul> <li>GitHub</li> <li>GitLab</li> <li>Gitea</li> <li>Forgejo</li> <li>Bitbucket Datacenter</li> </ul> <pre><code>CROW_FORGE=github # gitlab,gitea,forgejo,bitbucket\n</code></pre> <p>As Crow does not have its own user registry, users are provided from the linked forge via OAuth2. There is no way to register users manually.</p> <p>Registration is closed by default (<code>CROW_OPEN=false</code>). If registration is open (<code>CROW_OPEN=true</code>) then every user with an account at the configured forge can login and by this register an account.</p> <p>Warning</p> <p>It is highly recommended to restrict login to a specific subset of users, e.g. members of an organization. Otherwise, there is no control about who can register and create repositories to run pipelines. This is also recommended for private Crow instances. Org-based restrictions can be done by setting <code>CROW_ORGS</code>:</p> <pre><code>CROW_ORGS=org1\n</code></pre> <p>This would limit the use of Crow to member of this organization, allowing administrative control about users.</p>"},{"location":"configuration/server/#admin-users","title":"Admin users","text":"<pre><code>CROW_ADMIN=johnDoe,janeSmith\n</code></pre> <p>Besides setting this env var, admins can also be promoted manually in the UI by existing admins (<code>Settings -&gt; Users -&gt; Edit User</code>).</p>"},{"location":"configuration/server/#database","title":"Database","text":""},{"location":"configuration/server/#sqlite","title":"SQLite","text":"<p>Crow uses a SQLite database stored under <code>/var/lib/CROW/</code>.</p> <p>When using the docker-compose installation, make sure to persist (and backup) the volume serving this directory.</p>"},{"location":"configuration/server/#postgres","title":"Postgres","text":"<p>Postgres &gt;= 11 is currently supported.</p> <pre><code>CROW_DATABASE_DRIVER=postgres\nCROW_DATABASE_DATASOURCE=postgres://&lt;user&gt;:&lt;pw&gt;@0.0.0.0:5432/&lt;dbname&gt;?sslmode=disable\n</code></pre>"},{"location":"configuration/server/#mysqlmariadb","title":"MySQL/MariaDB","text":"<p>The minimum version of MySQL/MariaDB required is determined by the <code>go-sql-driver/mysql</code> - see its README for more information.</p> <pre><code>CROW_DATABASE_DRIVER=mysql\nCROW_DATABASE_DATASOURCE=&lt;user&gt;:&lt;pw&gt;@tcp(0.0.0.0:3306)/&lt;dbname&gt;?parseTime=true\n</code></pre>"},{"location":"configuration/server/#migration","title":"Migration","text":"<p>Crow automatically handles database migrations, i.e. when the schema changes in a new version, the changes are applied during version upgrade. This also means that reverting to an old version after such a migration is not supported.</p>"},{"location":"configuration/server/#backend","title":"Backend","text":"<p>The backend specifies how and where builds are executed. There are two production-grade backends:</p> <ul> <li><code>docker</code></li> <li><code>kubernetes</code></li> </ul> <p>In addition, the <code>local</code> backends exists which allows executing pipelines on the local machine that runs the agent directly.</p> <p>Warning</p> <p>the <code>local</code> backend is aimed at local debugging and development and should not be used in production.</p> <p>Info</p> <p>Podman is not supported as a backend and cannot be used as a backend engine for the \"docker\" backend. This backend needs a dedicated integration using the podman-specific golang libraries. Contributions welcome!</p>"},{"location":"configuration/server/#docker","title":"Docker","text":"<p>The <code>docker</code> backend can be seen as the \"default\" backend. It executes each step inside a separate container started on the agent.</p>"},{"location":"configuration/server/#backend-options","title":"Backend Options","text":"<p>The following <code>backend_options</code> are available:</p> <pre><code>steps:\n  - name: test\n  [...]\n    backend_options:\n      docker:\n        # same as 'docker run --user'\n        user:\n</code></pre>"},{"location":"configuration/server/#housekeeping","title":"Housekeeping","text":"<p>Crow will not automatically clean up images from the Docker installation on the host. Doing so needs extra configuration and should be included as a regular system maintenance task.</p> <p>Here are a few helper commands related to image cleanup:</p> <p>Removing all unused images:</p> <pre><code>docker image rm $(docker images --filter \"dangling=true\" -q --no-trunc)\n</code></pre> <p>Removing dangling/unused Crow volumes:</p> <pre><code>docker volume rm $(docker volume ls --filter name=^crow_* --filter dangling=true -q)\n</code></pre> <p>Pruning all images older than 30 days if the underlying file system partition has reached a certain size:</p> <pre><code># prune when disk usage &gt; 300GB\ndf -k | grep -E '/var/lib/docker$' | awk '{if ($3 &gt; 300000000) system(\"docker image prune -f -a --filter until=720h\")}'\n</code></pre>"},{"location":"configuration/server/#kubernetes","title":"Kubernetes","text":"<p>The Kubernetes backend executes steps inside standalone Pods. For every pipeline, a temporary PVC is created for the lifetime of the pipeline to transfer files between steps.</p> <p>Every step is executed in a distinct pod. This allows every step to be run on a different node, ensuring maximum flexibility with respect to resource distribution.</p> <p>Info</p> <p>There are plans to add a \"one pod per pipeline\" setting which executes all steps as containers of a single pod. This has the advantage of not requiring a RWX volume for file persistence between pods but would at the same time reduce the scheduling flexibility (as it would require the pod to be scheduled on a node that can take the step with the higher resource need).</p>"},{"location":"configuration/server/#private-registries","title":"Private Registries","text":"<p>Images from private registries require authentication. This is commonly done by providing a <code>imagePullSecret</code> in Kubernetes.</p> <p>To do so, the env var <code>CROW_BACKEND_K8S_PULL_SECRET_NAMES</code> can be used. It references existing Kubernetes secrets which contain registry pull secrets and allows the <code>agent</code> to make use of these in a specific namespace (defined via <code>CROW_BACKEND_K8S_NAMESPACE</code>) during pipeline creation.</p> <p>The Kubernetes secret must be of type <code>kubernetes.io/dockerconfigjson</code> and look like:</p> <pre><code>.dockerconfigjson: '{\"auths\":{\"https://index.docker.io/v1/\":{\"auth\":\"&lt;auth&gt;\",\"password\":\"&lt;pw&gt;\",\"username\":\"&lt;username&gt;\"}}}'\n</code></pre> <p>Tip</p> <p>The secret value can be obtained by issuing a local login to the registry and then extracting the created config value from <code>~/.docker/config.json</code>.</p>"},{"location":"configuration/server/#backend-options_1","title":"Backend options","text":"<p>The Kubernetes backend allows specifying requests and limits on a per-step basic, most commonly for CPU and memory.</p> <p>Note</p> <p>Adding a <code>resources</code> definition to all steps is essential for the Kubernetes backend to ensure efficient scheduling.</p>"},{"location":"configuration/server/#general","title":"General","text":"<pre><code>steps:\n  - name: 'Kubernetes step'\n    image: alpine\n    commands:\n      - echo \"Hello world\"\n    backend_options:\n      kubernetes:\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 100m\n          limits:\n            memory: 400Mi\n            cpu: 1000m\n</code></pre> <p>The following other common pod settings in Kubernetes are supported in <code>backend_options</code> using the same syntax as in Kubernetes:</p> <ul> <li>nodeSelector</li> <li>volumes</li> <li>tolerations</li> <li>securityContext</li> <li>annotations</li> <li>labels</li> </ul> <p>If your unfamiliar with some, please check the corresponding upstream documentation in Kubernetes.</p>"},{"location":"configuration/server/#noteworthy-options-and-adaptations","title":"Noteworthy options and adaptations","text":"<p>In the following, special/uncommon settings are explained in more detail.</p> <ul> <li> <p>Limit Ranges can be used to set the limits by per-namespace basis.</p> </li> <li> <p><code>runtimeClassName</code> specifies the name of the <code>RuntimeClass</code> which will be used to run the Pod.   If <code>runtimeClassName</code> is not set, the default <code>RuntimeHandler</code> will be used.   See the Kubernetes documentation on runtime classes for more information.</p> </li> <li> <p>To allow annotations and labels, the env var <code>crow_BACKEND_K8S_POD_ANNOTATIONS_ALLOW_FROM_STEP</code> and/or <code>crow_BACKEND_K8S_POD_LABELS_ALLOW_FROM_STEP</code> must be set to <code>true</code>.</p> </li> <li> <p>Users of crio-o need to configure the workspace for all pipelines centrally in order ensure correct execution (see this issue):</p> </li> </ul> <pre><code>workspace:\nbase: '/crow'\npath: '/'\n</code></pre>"},{"location":"configuration/server/#local","title":"Local","text":"<p>Danger</p> <p>The local backend executes pipelines on the local system without any isolation.</p> <p>Note</p> <p>This backend does not support 'services'.</p> <p>This backend should only be used in private environments where the code and pipeline can be trusted. It should not be used for public instances where unknown users can add new repositories and execute code. The agent should not run as a privileged user (root).</p> <p>The backend will use a random directory in <code>$TMPDIR</code> to store the cloned code and execute commands.</p> <p>In order to use this backend, a binary of the agent must be built and made executable on the respective server.</p> <p>The backend can be used by setting</p> <pre><code>CROW_BACKEND=local\n</code></pre> <p>The entrypoint of the <code>image:</code> definition in the pipeline is used to declare the shell, such as <code>bash</code> or <code>fish</code>.</p> <p>Plugins are supported and executed as expected. In the context of the <code>local</code> backend, plugins are effectively executable binaries, which can be located using their name if available in <code>$PATH</code>, or through an absolute path:</p> <pre><code>steps:\n  - name: build\n    image: /usr/bin/tree\n</code></pre>"},{"location":"configuration/server/#metrics","title":"Metrics","text":""},{"location":"configuration/server/#endpoint","title":"Endpoint","text":"<p>Crow exposes a Prometheus-compatible <code>/metrics</code> endpoint protected by the environment variable <code>CROW_PROMETHEUS_AUTH_TOKEN</code>.</p> <pre><code>global:\n  scrape_interval: 60s\n\nscrape_configs:\n  - job_name: 'crow'\n    bearer_token: dummyToken...\n\n    static_configs:\n      - targets: ['crow.domain.com']\n</code></pre>"},{"location":"configuration/server/#authorization","title":"Authorization","text":"<p>To access the endpoint, a user-level API token must be created by an admin and handed over in the Prometheus configuration file as a bearer token:</p> <pre><code> global:\n   scrape_interval: 60s\n\n scrape_configs:\n   - job_name: 'crow'\n+    bearer_token: &lt;token&gt;\n\n     static_configs:\n        - targets: ['crow.domain.com']\n</code></pre> <p>Alternatively, the token can be read from a file:</p> <pre><code> global:\n   scrape_interval: 60s\n\n scrape_configs:\n   - job_name: 'crow'\n+    bearer_token_file: /etc/secrets/crow-monitoring-token\n\n     static_configs:\n        - targets: ['crow.domain.com']\n</code></pre>"},{"location":"configuration/server/#reference","title":"Reference","text":"<p>The following metrics are exposed:</p> <pre><code># HELP crow_pipeline_count Pipeline count.\n# TYPE crow_pipeline_count counter\ncrow_pipeline_count{branch=\"main\",pipeline=\"total\",repo=\"crow-ci/crow\",status=\"success\"} 3\ncrow_pipeline_count{branch=\"dev\",pipeline=\"total\",repo=\"crow-ci/crow\",status=\"success\"} 3\n# HELP crow_pipeline_time Build time.\n# TYPE crow_pipeline_time gauge\ncrow_pipeline_time{branch=\"main\",pipeline=\"total\",repo=\"crow-ci/crow\",status=\"success\"} 116\ncrow_pipeline_time{branch=\"dev\",pipeline=\"total\",repo=\"crow-ci/crow\",status=\"success\"} 155\n# HELP crow_pipeline_total_count Total number of builds.\n# TYPE crow_pipeline_total_count gauge\ncrow_pipeline_total_count 1025\n# HELP crow_pending_steps Total number of pending pipeline steps.\n# TYPE crow_pending_steps gauge\ncrow_pending_steps 0\n# HELP crow_repo_count Total number of repos.\n# TYPE crow_repo_count gauge\ncrow_repo_count 9\n# HELP crow_running_steps Total number of running pipeline steps.\n# TYPE crow_running_steps gauge\ncrow_running_steps 0\n# HELP crow_user_count Total number of users.\n# TYPE crow_user_count gauge\ncrow_user_count 1\n# HELP crow_waiting_steps Total number of pipeline waiting on deps.\n# TYPE crow_waiting_steps gauge\ncrow_waiting_steps 0\n# HELP crow_worker_count Total number of workers.\n# TYPE crow_worker_count gauge\ncrow_worker_count 4\n</code></pre>"},{"location":"configuration/server/#ssl","title":"SSL","text":"<p>Tip</p> <p>Using a reverse proxy instead of configuring SSL within Crow provides more flexibility and security and adheres more to \"best practices\" for deploying applications. </p> <p>To enable Crow to terminate TLS directly, the following settings are available:</p> <pre><code>CROW_SERVER_CERT=/etc/certs/crow.example.com/server.crt\nCROW_SERVER_KEY=/etc/certs/crow.example.com/server.key\n</code></pre> <p>SSL support is provided using the ListenAndServeTLS function from the Go standard library.</p>"},{"location":"configuration/server/#container-configuration","title":"Container configuration","text":"<p>In addition to the ports shown in the docker-compose installation, port 443 must be exposed:</p> <pre><code> services:\n   crow-server:\n     [...]\n     ports:\n+      - 80:80\n+      - 443:443\n       - 9000:9000\n</code></pre> <p>Additionally, the certificate and key must be mounted and referenced:</p> <pre><code> services:\n   crow-server:\n     environment:\n+      - CROW_SERVER_CERT=/etc/certs/crow.example.com/server.crt\n+      - CROW_SERVER_KEY=/etc/certs/crow.example.com/server.key\n     volumes:\n+      - /etc/certs/crow.example.com/server.crt:/etc/certs/crow.example.com/server.crt\n+      - /etc/certs/crow.example.com/server.key:/etc/certs/crow.example.com/server.key\n</code></pre>"},{"location":"configuration/server/#logging","title":"Logging","text":"<p>The following env vars apply to logging configuration:</p> <pre><code># default 'info'\nWOODPECKER_LOG_LEVEL\n# default 'stderr'\nWOODPECKER_LOG_FILE\n# default 'false'\nWOODPECKER_DATABASE_LOG\n# default 'false'\nWOODPECKER_DATABASE_LOG_SQL\n# default 'database'\nWOODPECKER_LOG_STORE\n# not set\nWOODPECKER_LOG_STORE_FILE_PATH\n</code></pre>"},{"location":"configuration/server/#server-agent","title":"Server &amp; Agent","text":"<p>By default, Crow streams the server and agent output to <code>stderr</code> and does not persist it.</p> <p>Note</p> <p>There is a difference between Server/Agent logs and Pipeline logs. The former are the logs describing the application runtime itself, the latter are the logs from the executed pipelines.</p> <p>Setting <code>WOODPECKER_LOG_FILE</code> alongside with <code>WOODPECKER_LOG_STORE_FILE_PATH</code> enables file-based logging.</p> <p>If <code>WOODPECKER_DATABASE_LOG=true</code> is set, logs are written into the configured database.</p> <p>Warning</p> <p>Database logging might quickly increase the size of the DB, depending on the chosen log level. It is recommended to use file-based logging with automatic log-rotation (not configured automatically).</p>"},{"location":"configuration/server/#pipeline-logs","title":"Pipeline logs","text":"<p>Pipeline execution logs are stored by default alongside a pipeline configuration in the configured database. Depending on the amount of pipelines and their output, this can fill up the database over time.</p> <p>An alternative is store logs in an external file which can possibly be auto-rotated:</p> <pre><code>WOODPECKER_LOG_STORE=file\nWOODPECKER_LOG_STORE_FILE_PATH=&lt;path&gt;\n</code></pre> <p>Info</p> <p>Support for external S3-based logging is planned.</p>"},{"location":"configuration/server/#external-configuration-api","title":"External configuration API","text":""},{"location":"configuration/server/#introduction","title":"Introduction","text":"<p>To provide additional management and preprocessing capabilities for pipeline configurations, Crow supports an HTTP API which can be enabled to call an external config service.</p> <p>Before the run or restart of any pipeline Crow will make a POST request to an external HTTP API sending the current repository, build information and all current config files retrieved from the repository. The external API can then send back new pipeline configurations that will be used immediately or respond with HTTP 204 to tell the system to use the existing configuration.</p> <p>Every request sent by Crow is signed using a http-signature by a private key (ed25519) generated on the first start of the Crow server. You can get the public key for the verification of the http-signature from http(s)://your-woodpecker-server/api/signature/public-key.</p> <p>A simplistic example configuration service can be found here.</p> <p>Warning</p> <p>The external config service must be trusted as it is receiving secret information about the repository and pipeline and has the ability to change pipeline configs that could potentially execute malicious tasks.</p>"},{"location":"configuration/server/#configuration","title":"Configuration","text":"<pre><code>CROW_CONFIG_SERVICE_ENDPOINT=https://example.com/ciconfig\n</code></pre> <p>Exemplary request from Crow:</p> <pre><code>{\n  \"repo\": {\n    \"id\": 100,\n    \"uid\": \"\",\n    \"user_id\": 0,\n    \"namespace\": \"\",\n    \"name\": \"woodpecker-test-pipe\",\n    \"slug\": \"\",\n    \"scm\": \"git\",\n    \"git_http_url\": \"\",\n    \"git_ssh_url\": \"\",\n    \"link\": \"\",\n    \"default_branch\": \"\",\n    \"private\": true,\n    \"visibility\": \"private\",\n    \"active\": true,\n    \"config\": \"\",\n    \"trusted\": false,\n    \"protected\": false,\n    \"ignore_forks\": false,\n    \"ignore_pulls\": false,\n    \"cancel_pulls\": false,\n    \"timeout\": 60,\n    \"counter\": 0,\n    \"synced\": 0,\n    \"created\": 0,\n    \"updated\": 0,\n    \"version\": 0\n  },\n  \"pipeline\": {\n    \"author\": \"myUser\",\n    \"author_avatar\": \"https://myforge.com/avatars/d6b3f7787a685fcdf2a44e2c685c7e03\",\n    \"author_email\": \"my@email.com\",\n    \"branch\": \"main\",\n    \"changed_files\": [\"some-file-name.txt\"],\n    \"commit\": \"2fff90f8d288a4640e90f05049fe30e61a14fd50\",\n    \"created_at\": 0,\n    \"deploy_to\": \"\",\n    \"enqueued_at\": 0,\n    \"error\": \"\",\n    \"event\": \"push\",\n    \"finished_at\": 0,\n    \"id\": 0,\n    \"link_url\": \"https://myforge.com/myUser/woodpecker-testpipe/commit/2fff90f8d288a4640e90f05049fe30e61a14fd50\",\n    \"message\": \"test old config\\n\",\n    \"number\": 0,\n    \"parent\": 0,\n    \"ref\": \"refs/heads/main\",\n    \"refspec\": \"\",\n    \"clone_url\": \"\",\n    \"reviewed_at\": 0,\n    \"reviewed_by\": \"\",\n    \"sender\": \"myUser\",\n    \"signed\": false,\n    \"started_at\": 0,\n    \"status\": \"\",\n    \"timestamp\": 1645962783,\n    \"title\": \"\",\n    \"updated_at\": 0,\n    \"verified\": false\n  },\n  \"netrc\": {\n    \"machine\": \"https://example.com\",\n    \"login\": \"user\",\n    \"password\": \"password\"\n  }\n}\n</code></pre> <p>Exemplary response:</p> <pre><code>{\n  \"configs\": [\n    {\n      \"name\": \"central-override\",\n      \"data\": \"steps:\\n  - name: backend\\n    image: alpine\\n    commands:\\n      - echo \\\"Hello there from ConfigAPI\\\"\\n\"\n    }\n  ]\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#general","title":"General","text":"<p>Crow consists out of two parts: the \"server\" and the \"agent\".</p> <p>The server provides the UI, handles webhook requests to the underlying forge, serves the API and parses the pipeline configurations from the YAML files.</p> <p>The agent executes the pipelines using a specific backend (docker, kubernetes, local) and connects to the server via GRPC. Multiple agents can coexist besides each other, allowing to fine-tune job limits, backend choice and other agent-related settings for a single instance.</p> <p>Crow ships and uses a SQLite DB by default. For larger instances it is recommended to use it with a Postgres or MariaDB instance.<sup>1</sup></p> <p>Note</p> <p>The deployment of an external database is not covered here. There are many existing public guides for deploying databases. An alternative option is to use a managed DB service from a Cloud provider. If you are unsure what you need and if Crow is a good fit for you in general, you can also proceed with the SQLite DB first and decide later.</p> <p>There are currently three official ways how to install Crow (via <code>docker-compose</code> or distribution-specific binaries on single servers and via <code>helm</code> on Kubernetes. The development of an Ansible role is in the works.</p> <p>Besides, custom approaches might be available in the community.</p>"},{"location":"installation/#crow-agent-secret","title":"Crow agent secret","text":"<p>To allow secure communication between the server and agent via GRPC, a token is required.</p> <p>There are two ways to register new agents:</p> <ul> <li>Via a system token</li> <li>Via an agent token</li> </ul> <p>When using the helm chart, a Kubernetes secret containing an agent token is created automatically.  This token is then used by all agents which have access to this secret. In the best case, no further configuration is required.</p>"},{"location":"installation/#system-token","title":"System token","text":"<p>The system token is set via the env var <code>CROW_AGENT_SECRET</code> for both server and agent. There can only ever be one system token at the same time, therefore.</p> <p>If a system token is set, the registration process is as follows:</p> <ol> <li>The first time the agent communicates with the server, it is using the system token</li> <li>The server registers the agent in its database if not done before and generates a unique ID which is then sent back to the agent</li> <li>The agent stores the received ID in a config file (path is defined by <code>CROW_AGENT_CONFIG_FILE</code>)</li> <li>At the subsequent starts of the agent, it uses this token and its received ID to identify itself to the server</li> </ol> <p>Info</p> <p>If the ID is not stored/persisted in <code>CROW_AGENT_CONFIG_FILE</code> and the agent connects with a matching agent token, a new agent is registered in the DB and the UI. This will happen every time the agent container is restarted. While this is not an issue at runtime, the list of registered agents will grow and leave behind \"zombie\" agents which are registered in the DB but not active anymore. It is therefore recommended to persist <code>CROW_AGENT_CONFIG_FILE</code> to ensure idempotent agent registrations.</p>"},{"location":"installation/#agent-token","title":"Agent token","text":"<p>Agent tokens are created in the UI of the server (<code>Settings -&gt; Agents -&gt; Add agent</code>). This is an alternative way to tell the server about persistent agents and their possible connections.</p> <p>Agent tokens can be handed over to individual agents via <code>crow_AGENT_SECRET</code>, making it possible to register additional unique agents.</p> <p></p> <p>Registration of a new agent through UI</p>"},{"location":"installation/#image-tags","title":"Image tags","text":"<p>Info</p> <p>There is no <code>latest</code> tag on purpose to prevent accidental major version upgrades. Either use a Semver tag or one of the rolling major/minor version tags.</p> <ul> <li><code>vX.Y.Z</code>: SemVer tags for specific releases, no entrypoint shell (scratch image)</li> <li><code>vX.Y</code></li> <li><code>vX</code></li> <li><code>vX.Y.Z-alpine</code>: SemVer tags for specific releases, based on Alpine, rootless for Server and CLI (as of v3.0).</li> <li><code>vX.Y-alpine</code></li> <li><code>vX-alpine</code></li> <li><code>next</code>: Built from the <code>main</code> branch</li> <li><code>pull_&lt;PR_ID&gt;</code>: Images built from Pull Request branches.</li> </ul>"},{"location":"installation/#image-registries","title":"Image registries","text":"<p>Images are pushed to the GitHub Container Registry (ghcr.io) of the project.</p> <p>Info</p> <p>If there funds available to cover a DockerHub team subscription, images will also be published to DockerHub.</p> <ul> <li> <p>crow-server (ghcr.io)</p> </li> <li> <p>crow-agent (ghcr.io)</p> </li> <li> <p>crow-cli (ghcr.io)</p> </li> <li> <p>crow-autoscaler (ghcr.io)</p> </li> </ul> <ol> <li> <p>This is primarily because Crow (still) stores pipeline logs in the DB. A refactoring to storing these outside the DB by default is planned but now yet implemented.\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/docker-compose/","title":"docker-compose","text":"<p>This exemplary docker-compose setup shows the deployment of a Crow instance connected to GitHub (<code>CROW_GITHUB=true</code>). If you are using another forge, please change this including the respective secret settings.</p> <p>It creates persistent volumes for the server and agent config directories. The bundled SQLite DB is stored in <code>/var/lib/crow</code> and is the most important part to be persisted as it holds all users and repository information.</p> <p>The server uses the default port 8000 and gets exposed to the host here, so Crow can be accessed through this port on the host or by a reverse proxy sitting in front of it.</p> <pre><code>services:\n  crow-server:\n    image: ghcr.io/crowci/crow-server:latest\n    ports:\n      - 8000:8000\n    volumes:\n      - crow-server:/var/lib/crow/\n    environment:\n      - CROW_OPEN=true\n      - CROW_HOST=${CROW_HOST}\n      - CROW_GITHUB=true\n      - CROW_GITHUB_CLIENT=${CROW_GITHUB_CLIENT}\n      - CROW_GITHUB_SECRET=${CROW_GITHUB_SECRET}\n      - CROW_AGENT_SECRET=${CROW_AGENT_SECRET}\n\n  agent:\n    image: ghcr.io/crowci/crow-agent:latest\n    restart: always\n    depends_on:\n      - crow-server\n    volumes:\n      - crow-agent:/etc/crow\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - CROW_SERVER=crow-server:9000\n      - CROW_AGENT_SECRET=${CROW_AGENT_SECRET}\n</code></pre>"},{"location":"installation/helm/","title":"Helm","text":""},{"location":"installation/helm/#installation","title":"Installation","text":"<p>Crow provides a helm chart for Kubernetes environments:</p> <pre><code>helm repo add crowci oci://ghcr.io/crowci/helm\nhelm install crow crowci/crow\n</code></pre>"},{"location":"installation/helm/#metrics","title":"Metrics","text":"<p>To enable metrics gathering, set the following in <code>values.yml</code>:</p> <pre><code>metrics:\n  enabled: true\n  port: 9001\n</code></pre> <p>This activates the <code>/metrics</code> endpoint on port 9001 without authentication. This port is not exposed externally by default. Use the instructions at Prometheus if you want to enable authenticated external access to metrics.</p> <p>To enable both Prometheus pod monitoring discovery, set:</p> <pre><code>prometheus:\n  podmonitor:\n    enabled: true\n    interval: 60s\n    labels: {}\n</code></pre> <p>If you are not receiving metrics after following the steps above, verify that your Prometheus configuration includes your namespace explicitly in the <code>podMonitorNamespaceSelector</code> or that the selectors are disabled:</p> <pre><code># Search all available namespaces\npodMonitorNamespaceSelector:\n  matchLabels: {}\n# Enable all available pod monitors\npodMonitorSelector:\n  matchLabels: {}\n</code></pre>"},{"location":"installation/image-variants/","title":"Image variants","text":""},{"location":"installation/image-variants/#image-tags","title":"Image tags","text":"<p>Info</p> <p>There is no <code>latest</code> tag on purpose to prevent accidental major version upgrades. Either use a Semver tag or one of the rolling major/minor version tags.</p> <ul> <li><code>vX.Y.Z</code>: SemVer tags for specific releases, no entrypoint shell (scratch image)</li> <li><code>vX.Y</code></li> <li><code>vX</code></li> <li><code>vX.Y.Z-alpine</code>: SemVer tags for specific releases, based on Alpine, rootless for Server and CLI (as of v3.0).</li> <li><code>vX.Y-alpine</code></li> <li><code>vX-alpine</code></li> <li><code>next</code>: Built from the <code>main</code> branch</li> <li><code>pull_&lt;PR_ID&gt;</code>: Images built from Pull Request branches.</li> </ul>"},{"location":"installation/image-variants/#image-registries","title":"Image registries","text":"<p>Images are pushed to the GitHub Container Registry (ghcr.io) of the project.</p> <p>Info</p> <p>If there funds available to cover a DockerHub team subscription, images will also be published to DockerHub.</p> <ul> <li> <p>crow-server (ghcr.io)</p> </li> <li> <p>crow-agent (ghcr.io)</p> </li> <li> <p>crow-cli (ghcr.io)</p> </li> <li> <p>crow-autoscaler (ghcr.io)</p> </li> </ul>"},{"location":"installation/proxy/","title":"Proxy Setup","text":"<p>In the following, different reverse proxy setups are given (in alphabetical order) to make Crow work behind a reverse proxy:</p>"},{"location":"installation/proxy/#apache","title":"Apache","text":"<p>The following modules are required:</p> <ul> <li><code>proxy</code></li> <li><code>proxy_http</code></li> </ul> <pre><code>ProxyPreserveHost On\n\nRequestHeader set X-Forwarded-Proto \"https\"\n\nProxyPass / http://127.0.0.1:8000/\nProxyPassReverse / http://127.0.0.1:8000/\n</code></pre>"},{"location":"installation/proxy/#caddy","title":"Caddy","text":"<pre><code># WebUI and API\ncrow.example.com {\n  reverse_proxy crow-server:8000\n}\n\n# expose gRPC\ncrow-agent.example.com {\n  reverse_proxy h2c://crow-server:9000\n}\n</code></pre> <p>Info</p> <p>The above configuration shows how to create reverse-proxies for server and agent communication. If the agent is configured to use SSL, do not forget to enable <code>CROW_GRPC_SECURE</code>.</p>"},{"location":"installation/proxy/#nginx","title":"Nginx","text":"<pre><code>server {\n    listen 80;\n    server_name crow.example.com;\n\n    location / {\n        proxy_set_header X-Forwarded-For $remote_addr;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $http_host;\n\n        proxy_pass http://127.0.0.1:8000;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_buffering off;\n\n        chunked_transfer_encoding off;\n    }\n}\n</code></pre> <p>Info</p> <p>This does not cover an SSL configuration with NGINX but only shows how to properly forward incoming requests through NGINX to Crow.</p>"},{"location":"installation/proxy/#ngrok","title":"Ngrok","text":"<p>Start <code>ngrok</code> using the designed Crow port, e.g. <code>ngrok http 8000</code>. This will return a response similar to the following</p> <p>Set <code>CROW_HOST</code> to the returned URL and (re)start Crow.</p>"},{"location":"installation/proxy/#tunnelmole","title":"Tunnelmole","text":"<p>Start tunnelmole using the designed Crow port, e.g. <code>tmole 8000</code>. This will return a response similar to the following</p> <pre><code>tmole 8000\nhttp://bvdo5f-ip-49-183-170-144.tunnelmole.net is forwarding to localhost:8000\nhttps://bvdo5f-ip-49-183-170-144.tunnelmole.net is forwarding to localhost:8000\n</code></pre> <p>Set <code>CROW_HOST</code> to the returned URL (e.g. exx.tunnelmole.net) and (re)start Crow.</p>"},{"location":"installation/proxy/#traefik","title":"Traefik","text":"<p>To install the crow server behind a Traefik load balancer, both the http and the gRPC ports must be exposed and configured.</p> <p>Here is a comprehensive example, which uses <code>traefik</code> running via docker compose and applies TLS termination and automatic redirection from http to https.</p> <pre><code>services:\n  server:\n    image: ghcr.io/crowci/crow-server:latest\n    environment:\n      # Crow settings ...\n\n    networks:\n      - dmz # externally defined network, so that traefik can connect to the server\n    volumes:\n      - crow-server-data:/var/lib/crow/\n\n    deploy:\n      labels:\n        - traefik.enable=true\n\n        # web server\n        - traefik.http.services.crow-service.loadbalancer.server.port=8000\n\n        - traefik.http.routers.crow-secure.rule=Host(`cd.your-domain.com`)\n        - traefik.http.routers.crow-secure.tls=true\n        - traefik.http.routers.crow-secure.tls.certresolver=letsencrypt\n        - traefik.http.routers.crow-secure.entrypoints=web-secure\n        - traefik.http.routers.crow-secure.service=crow-service\n\n        - traefik.http.routers.crow.rule=Host(`cd.your-domain.com`)\n        - traefik.http.routers.crow.entrypoints=web\n        - traefik.http.routers.crow.service=crow-service\n\n        - traefik.http.middlewares.crow-redirect.redirectscheme.scheme=https\n        - traefik.http.middlewares.crow-redirect.redirectscheme.permanent=true\n        - traefik.http.routers.crow.middlewares=crow-redirect@docker\n\n        #  gRPC service\n        - traefik.http.services.crow-grpc.loadbalancer.server.port=9000\n        - traefik.http.services.crow-grpc.loadbalancer.server.scheme=h2c\n\n        - traefik.http.routers.crow-grpc-secure.rule=Host(`crow-grpc.your-domain.com`)\n        - traefik.http.routers.crow-grpc-secure.tls=true\n        - traefik.http.routers.crow-grpc-secure.tls.certresolver=letsencrypt\n        - traefik.http.routers.crow-grpc-secure.entrypoints=web-secure\n        - traefik.http.routers.crow-grpc-secure.service=crow-grpc\n\n        - traefik.http.routers.crow-grpc.rule=Host(`crow-grpc.your-domain.com`)\n        - traefik.http.routers.crow-grpc.entrypoints=web\n        - traefik.http.routers.crow-grpc.service=crow-grpc\n\n        - traefik.http.middlewares.crow-grpc-redirect.redirectscheme.scheme=https\n        - traefik.http.middlewares.crow-grpc-redirect.redirectscheme.permanent=true\n        - traefik.http.routers.crow-grpc.middlewares=crow-grpc-redirect@docker\n\nnetworks:\n  dmz:\n    external: true\n</code></pre>"}]}